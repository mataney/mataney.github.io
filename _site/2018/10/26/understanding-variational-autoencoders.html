<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <!-- Main JS (navbar.js, katex_init.js and masonry_init.js)-->
    <script defer=true src="/assets/js/main.min.js"></script>
    
    <!-- CSS -->
    <link rel="stylesheet" href="/assets/css/main.css">

    <!--Favicon-->
    <link rel="shortcut icon" href="/assets/img/my_cartoon.png" type="image/x-icon">

    <!-- Canonical -->
    <link rel="canonical" href="http://localhost:4000/2018/10/26/understanding-variational-autoencoders.html">

    <!-- RSS -->
    <link rel="alternate" type="application/atom+xml" title="Matan Eyal" href="http://localhost:4000///feed.xml"/>

    <!-- Font Awesome -->
    <!-- <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"> -->
    <link rel="stylesheet" type="text/css" href="/assets/css/vendor/font-awesome.min.css">
    
    

    <!-- Google Fonts -->
    
    <link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic" rel="stylesheet" type="text/css"> 
    

    <!-- KaTeX 0.8.3 -->
    
    <link rel="stylesheet" type="text/css" href="/assets/css/vendor/katex.min.css">
    <script src="/assets/js/vendor/katex.min.js">
    </script>
    

    <!-- Google Analytics -->
    
    
    <!-- seo tags -->
    <!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Understanding Variational Autoencoders | Matan Eyal</title>
<meta name="generator" content="Jekyll v3.8.4" />
<meta property="og:title" content="Understanding Variational Autoencoders" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In this post I attempt to describe Variational Autoencoders (VAE) both from a theoretical and a practical point of view. The first paper to introduce VAE [Kingma et al. 2014] assumes the reader knows quite a bit of Variational Inference (VI). For those of us lacking this consept, i will take the same route the authors took in the original paper, but we will take a closer look on VI. Then, we will review the intersection of autoencoders and VI, the variational autoencoder. But first, let’s start with a really quick overview of vanilla autoencoders" />
<meta property="og:description" content="In this post I attempt to describe Variational Autoencoders (VAE) both from a theoretical and a practical point of view. The first paper to introduce VAE [Kingma et al. 2014] assumes the reader knows quite a bit of Variational Inference (VI). For those of us lacking this consept, i will take the same route the authors took in the original paper, but we will take a closer look on VI. Then, we will review the intersection of autoencoders and VI, the variational autoencoder. But first, let’s start with a really quick overview of vanilla autoencoders" />
<link rel="canonical" href="http://localhost:4000/2018/10/26/understanding-variational-autoencoders.html" />
<meta property="og:url" content="http://localhost:4000/2018/10/26/understanding-variational-autoencoders.html" />
<meta property="og:site_name" content="Matan Eyal" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-10-26T00:00:00+03:00" />
<script type="application/ld+json">
{"description":"In this post I attempt to describe Variational Autoencoders (VAE) both from a theoretical and a practical point of view. The first paper to introduce VAE [Kingma et al. 2014] assumes the reader knows quite a bit of Variational Inference (VI). For those of us lacking this consept, i will take the same route the authors took in the original paper, but we will take a closer look on VI. Then, we will review the intersection of autoencoders and VI, the variational autoencoder. But first, let’s start with a really quick overview of vanilla autoencoders","@type":"BlogPosting","url":"http://localhost:4000/2018/10/26/understanding-variational-autoencoders.html","headline":"Understanding Variational Autoencoders","dateModified":"2018-10-26T00:00:00+03:00","datePublished":"2018-10-26T00:00:00+03:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2018/10/26/understanding-variational-autoencoders.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <!-- Manual seo tags -->
    <!--
    <title>Understanding Variational Autoencoders | Matan Eyal</title>
    <meta name="description" content="In this post I attempt to describe Variational Autoencoders (VAE) both from a theoretical and a practical point of view. The first paper to introduce VAE [Ki...">
    -->
</head>

  <body>
    <header class="site-header">
    
    <!-- Logo and title -->
	<div class="branding">
		<a href="/">
			<img class="avatar" src="/assets/img/my_cartoon.png" alt=""/>
		</a>

		<h1 class="site-title">
			<a href="/">Matan Eyal</a>
		</h1>
	</div>
    
    <!-- Toggle menu -->
    <nav class="clear">
    <a id="pull" class="toggle" href="#">
    <i class="fa fa-bars fa-lg"></i>
    </a>
    
    <!-- Menu -->
    <ul class="hide">
        <!-- Auto Generation of NORMAL pages in the navbar -->
        
        
        
        
        
        
        
        <li class="separator"> | </li>
        <li>
            <a class="clear" href="/about/">
                About
            </a>
        </li>
        
        
        
        
        
        
        
        
        
        
        
         
        
        <!-- Auto Generation of SPECIAL pages in the navbar -->
        
    </ul>
        
	</nav>
</header>

    <div class="content">
      <article >
  <header id="main" style="background-image: url('/')">
    <h1 id="Understanding+Variational+Autoencoders" class="title">Understanding Variational Autoencoders</h1>
    <p class="meta">
    October 26, 2018
    
    </p>
  </header>
  <section class="post-content">
  
    <p>In this post I attempt to describe Variational Autoencoders (VAE) both from a theoretical and a practical point of view. The first paper to introduce VAE [Kingma et al. 2014] assumes the reader knows quite a bit of Variational Inference (VI). For those of us lacking this consept, i will take the same route the authors took in the original paper, but we will take a closer look on VI. Then, we will review the intersection of autoencoders and VI, the variational autoencoder.
But first, let’s start with a really quick overview of vanilla autoencoders</p>

<h1 id="autoencoders">Autoencoders</h1>

<p>Generally speaking, an autoencoder is an unsupervised method designed to reconstruct some input data <script type="math/tex">\textbf{x} \in X</script>. The main feature of the autoencoder is the ability to learn a concise representation of the features of <script type="math/tex">\textbf{x}</script>, <i>i.e.</i>, <script type="math/tex">\textbf{z} \in Z</script>, while being able to reconstruct <script type="math/tex">\textbf{x}</script> from <script type="math/tex">\textbf{z}</script>. In order to create the reconstruction <script type="math/tex">\textbf{x}'</script> the autoencoder uses two functions. The first, an encoder, denoted by <script type="math/tex">q_\phi(\textbf{x} \mid \textbf{z})</script>, and a decoder, denoted by <script type="math/tex">p_\theta(\textbf{z} \mid \textbf{x})</script>, with parameters <script type="math/tex">\phi,~\theta</script> respectively. Notice that <script type="math/tex">% <![CDATA[
\dim(\textbf{z}) < \dim(\textbf{x}) %]]></script> otherwise the model will not learn anything of use. We want to minimize the reconstruction loss using some loss function, for example the MSE:</p>

<script type="math/tex; mode=display">(1) \qquad \phi, \theta = \arg\,min_{\phi, \theta} \| \textbf{x} - p_\theta(q_\phi(\textbf{x}))\|^2</script>

<p>From a coding theory perspective, the encoder takes the input, or observed variable, <script type="math/tex">\textbf{x} \in X</script> and maps it, in our case, using a Multi Layer Perceptron (MLP), to a latent or hidden variable, or code, <script type="math/tex">\textbf{z} \in Z</script>. Then the decoder maps <script type="math/tex">\textbf{z}</script>, again, using an MLP, to the reconstruction, <script type="math/tex">\textbf{x}'</script>.</p>

<p>Throughout reading this post, lets think of <script type="math/tex">X \sim p_{\theta^*}(\textbf{x})</script> as a distribution of digit drawings, like MNIST, as the ones below. So the autoencoder’s goal is to get <script type="math/tex">\textbf{x}</script>, an image of a digit, “shrink” its representation to a smaller dimention <script type="math/tex">\textbf{z}</script>, then reconstructing <script type="math/tex">\textbf{x}</script>, while not losing (much) information.</p>

<p><img src="/assets/understanding-VAE/mnist.png" alt="" height="75%" width="75%" />
<br /><em>Shouldn’t be introduced, the MNIST dataset.</em></p>

<h1 id="method">Method</h1>
<h2 id="problem-scenario">Problem Scenario</h2>

<p>After reading the previous section, we might notice that autoencoders fit the latent variable model paradigm, in which we assume the observable data <script type="math/tex">\textbf{x} \in X</script> is generated by some random process involving the continuous latent variables <script type="math/tex">\textbf{z}</script>. <i>i.e.</i>, We generate <script type="math/tex">\textbf{z}</script> from the prior distribution <script type="math/tex">p_{\theta^*}(\textbf{z})</script>, then <script type="math/tex">\textbf{x}</script> is generated from the conditional distribution <script type="math/tex">p_{\theta^*}(\textbf{x} \mid \textbf{z})</script>. Where the distributions <script type="math/tex">p_{\theta}(\textbf{z})</script>, <script type="math/tex">p_{\theta}(\textbf{x} \mid \textbf{z})</script> depend on some parameters <script type="math/tex">\mathbf{\theta}</script>. Notice that we do not have any information regarding the real values of <script type="math/tex">\textbf{z}</script> or the optimum value of parameters <script type="math/tex">\theta^*</script>.</p>

<p>(A note regarding notation. A more verbose notation could be: <script type="math/tex">p_{\theta_1}(\textbf{z})</script>, <script type="math/tex">p_{\theta_2}(\textbf{x} \mid \textbf{z})</script> are parametric functions and the real values of <script type="math/tex">\theta = (\theta_1, \theta_2)</script> are unknown. We will use the former notation for consistency with Kingma et al.’s paper)</p>

<h3 id="generation-process">Generation Process</h3>

<p>In addition to the reconstruction goal defined in the autoencoders section, we would like to be able to execute a generation process: Assume you can generate <script type="math/tex">\textbf{z} \sim p(\textbf{z})</script>, generate <script type="math/tex">\textbf{x}</script> from <script type="math/tex">p_\theta(\textbf{x} \mid \textbf{z})</script>.</p>

<p>Why is this a generative process?</p>

<p>Again, think of <script type="math/tex">X</script> as a set of images. These images come from some distribution <script type="math/tex">p_{\theta^*}(\textbf{x})</script> we know nothing about. But if we can find the conditional distribution <script type="math/tex">p_{\theta}(\textbf{x} \mid \textbf{z})</script> (And we know <script type="math/tex">p_{\theta}(\textbf{z})</script>) we can generate images!</p>

<p><img src="/assets/understanding-VAE/generator2.png" alt="" />
<br /><em>The Generation Process</em></p>

<p>Specifically, we would like to find <script type="math/tex">\theta</script> such that will maximize <script type="math/tex">p_\theta(\textbf{x})</script> for training images <script type="math/tex">X</script>, via the generation process <script type="math/tex">p(\textbf{x} \mid \textbf{z})</script>: Optimize parameters <script type="math/tex">\theta</script> such that the generative process <script type="math/tex">p_\theta(\textbf{x} \mid \textbf{z})</script> will return <script type="math/tex">\textbf{x}'</script> that are as close to the <script type="math/tex">\textbf{x} \in X</script>.</p>

<script type="math/tex; mode=display">\max_\theta p_\theta(\textbf{x}) = \max_\theta \int_z p(\textbf{z})p_\theta(\textbf{x} \mid \textbf{z})</script>

<p>Great, to compute this integral we only need to go over all possible <script type="math/tex">Z</script> and maximize <script type="math/tex">p_\theta(\textbf{x} \mid \textbf{z})</script> - The thing is, this is intractable.</p>

<p>Think about a simpler case, assume <script type="math/tex">\textbf{z}</script> is a boolean vector in <script type="math/tex">d</script> dimensions, then there are <script type="math/tex">2^d</script> possible configurations of <script type="math/tex">\textbf{z}</script>, which is exponential in <script type="math/tex">d</script> and well too big to integrate over. Our case is much worse, <script type="math/tex">\textbf{z}</script> is continuous.</p>

<p>To recap, we want to be able to generate data using <script type="math/tex">p_\theta(\textbf{x} \mid \textbf{z})</script>, we understand we can learn a generative process by maximizing  <script type="math/tex">\max_\theta p_\theta(\textbf{x})</script>, but this is intractable. Let’s leave this for a bit, but don’t forget this is our goal, and discuss another problem that might help us with this later.</p>

<h1 id="sidetracking-introduction-to-variational-and-posterior-inference">Sidetracking, Introduction to Variational and Posterior Inference</h1>

<p>In VI, just as before, we think of the data distribution as the marginal distribution of this:</p>

<script type="math/tex; mode=display">p(\textbf{x}) = \int_z p(\textbf{x}, \textbf{z}) = \int_z p(\textbf{z}) p(\textbf{x} \mid \textbf{z})</script>

<p>In the previous section we wanted to infer about the likelihood <script type="math/tex">p(\textbf{x} \mid \textbf{z})</script> but we hit a brick wall. Let’s look at this from another point of view and try to infer about the posterior, <script type="math/tex">p(\textbf{z} \mid \textbf{x})</script>. In order to do this we will use VI, a method that approximates probability densities through optimization. The probability density we want to approximate is the posterior, the conditional distribution of the hidden variables given the observed variables.</p>

<script type="math/tex; mode=display">p(\textbf{z} \mid \textbf{x}) = \frac{p(\textbf{z}, \textbf{x})}{p(\textbf{x})}</script>

<p>The denominator, <script type="math/tex">p(\textbf{x})p(x)</script>, as we saw beforehand, is intractable. Variational Inference(VI) suggests to find another distribution that we know how to sample from and is close to the original posterior distribution, <script type="math/tex">p(\textbf{z} \mid \textbf{x})p(z \sim x)</script>.</p>

<p><img src="/assets/understanding-VAE/var-inf.png" alt="" />
<em><br />Image from <a href="https://www.youtube.com/watch?v=ogdv6dbvVQ">this</a> NIPS tutorial. Notice in this post we use <script type="math/tex">\phi</script> instead of <script type="math/tex">\nu</script> as the distribution’s parameters.</em></p>

<p>Formally, Variational Inference suggest that instead of trying to infer about something we don’t know how to compute, lets try to optimize the proximity of a new distribution <script type="math/tex">q_\phi(\textbf{z} \mid \textbf{x})</script> to the original posterior <script type="math/tex">p(\textbf{z} \mid \textbf{x})</script>. How does it work? Select a variational family of distributions over the latent variables we know how to sample from, denoted by <script type="math/tex">Q_\phi(\textbf{z})</script>. Choose some parametric distribution <script type="math/tex">q_\phi(\textbf{z} \mid \textbf{x}) \in Q_\phi(\textbf{z})</script>. Then, search for <script type="math/tex">\phi</script> that approximates <script type="math/tex">q_{\phi}(\textbf{z} \mid \textbf{x})</script> to <script type="math/tex">p(\textbf{z} \mid \textbf{x})</script>. Now we can approximating sampling from <script type="math/tex">p(\textbf{z} \mid \textbf{x})</script> by sampling from <script type="math/tex">q_{\phi}(\textbf{z} \mid \textbf{x})</script>.</p>

<p>For example, assume we want to find an approximation to <script type="math/tex">p(\textbf{z} \mid \textbf{x})</script> we hypothesize that it can be approximated by a normal distribution with some mean and variance. Denote <script type="math/tex">Q_\phi(\textbf{z})</script> as the family of normal distributions <script type="math/tex">\mathcal{N}(\phi_{\mu}, \phi_{\sigma^2})</script>. We are looking for <script type="math/tex">\phi^*</script> such that <script type="math/tex">q_{\phi^*}(z)=\mathcal{N}(\phi_{\mu^*}, \phi_{\sigma^*})</script>  is as close as possible to the real posterior, <script type="math/tex">p(\textbf{z} \mid \textbf{x})</script>.</p>

<p><img src="/assets/understanding-VAE/var-inf2.png" alt="" height="75%" width="75%" />
<em><br />The plot shows the original distribution (yellow) along with the Laplace (red) and variational (green) approximations. Image from [11]</em></p>

<p>Why is this easier then our original question? our goal now is optimizing and not inferring, and optimizing is something we do quite a lot, just use your favorite gradient decent algorithm.</p>

<p>(Another note regarding notation. In VI we indeed look for some distribution <script type="math/tex">q_\phi(\textbf{z})</script> approximating <script type="math/tex">p(\textbf{z} \mid \textbf{x})</script> that may or may not depend on input <script type="math/tex">x</script>. In Kingma et al.’s paper we also care about <script type="math/tex">q_\theta(\cdot)</script> with respect to the input (Think about the reconstruction process). For this reason we specifically look for a distribution that depends on the input, so from now on, we note this as <script type="math/tex">q_\phi(\textbf{z} \mid \textbf{x})</script>)</p>

<p>So our optimization problem now, for the posterior inference, is minimizing the proximity of  <script type="math/tex">q_\phi(\textbf{z} \mid \textbf{x})</script> and <script type="math/tex">p(\textbf{z} \mid \textbf{x})</script>. Usually (but not always) in VI, closeness is defined by the Kullback–Leibler divergence defined over distributions <script type="math/tex">p(x)</script> and <script type="math/tex">q(x)</script> by</p>

<script type="math/tex; mode=display">\begin{aligned}
D_{KL}(q \Vert p) = \int_x q(x) \log \frac{q(x)}{p(x)}
\end{aligned}</script>

<p>Or for the minimization problem of our two distributions:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\min_\phi D_{KL}(q_\phi(\textbf{z} \mid \textbf{x}) \Vert p(\textbf{z} \mid \textbf{x})) &= \int_{z} q_\phi (\textbf{z} \mid \textbf{x}) \log \frac{q_\phi(\textbf{z} \mid \textbf{x})}{p(\textbf{z} \mid \textbf{x})} dz
\end{aligned} %]]></script>

<p>The second input to the KL divergence is the same posterior distribution we want to find, so we can’t compute it. Let’s see if there’s anything else we can say about this equation.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
D_{KL}(q_\phi(\textbf{z} \mid \textbf{x}) \Vert p(\textbf{z} \mid \textbf{x})) &= \int_{z} q_\phi (\textbf{z} \mid \textbf{x}) \log \frac{q_\phi(\textbf{z} \mid \textbf{x})}{p(\textbf{z} \mid \textbf{x})} dz \\
    &= \int_{z} q_\phi (\textbf{z} \mid \textbf{x}) \log \frac{q_\phi(\textbf{z} \mid \textbf{x}) p(\textbf{x})}{p(\textbf{z} , \textbf{x})} dz \\
    &= \int_{z} q_\phi (\textbf{z} \mid \textbf{x}) (\log \frac{q_\phi(\textbf{z} \mid \textbf{x})}{p(\textbf{z} , \textbf{x})} + \log p(\textbf{x})) dz \\
    &= \int_{z} q_\phi (\textbf{z} \mid \textbf{x}) \log \frac{q_\phi(\textbf{z} \mid \textbf{x})}{p(\textbf{z} , \textbf{x})}dz + \int_{z} q_\phi (\textbf{z} \mid \textbf{x}) \log p(\textbf{x}) dz\\
    &= \int_{z} q_\phi (\textbf{z} \mid \textbf{x}) \log \frac{q_\phi(\textbf{z} \mid \textbf{x})}{p(\textbf{z} , \textbf{x})}dz + \log p(\textbf{x}) \int_{z} q_\phi (\textbf{z} \mid \textbf{x}) dz\\
    &=  \int_{z} q_\phi (\textbf{z} \mid \textbf{x}) \log \frac{q_\phi(\textbf{z} \mid \textbf{x})}{p(\textbf{z} , \textbf{x})}dz + \log p(\textbf{x})\\
\end{aligned} %]]></script>

<p>Define <script type="math/tex">L(\phi; \textbf{x})</script> to be the negation of the first term:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
L(\phi; \textbf{x}) = -\int_{z} q_\phi (\textbf{z} \mid \textbf{x}) \log \frac{q_\phi(\textbf{z} \mid \textbf{x})}{p(\textbf{z} , \textbf{x})}dz &= \int_{z} q_\phi (\textbf{z} \mid \textbf{x}) \log \frac{p(\textbf{z}, \textbf{x})}{q_\phi(\textbf{z} \mid \textbf{x})}dz\\
&= \int_{z} q_\phi (\textbf{z} \mid \textbf{x}) \log \frac{p(\textbf{z})p(\textbf{x} \mid \textbf{z})}{q_\phi(\textbf{z} \mid \textbf{x})}dz
\end{aligned} %]]></script>

<p>And we get:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
    \log p(\textbf{x}) &= D_{KL}(q_\phi(\textbf{z} \mid \textbf{x}) \Vert P(\textbf{z} \mid \textbf{x})) + L(\phi; \textbf{x}) \\
\end{aligned} %]]></script>

<p>Do you remember I said Kingma et al. assumes quite a bit about the readers knowledge in VI, this is (almost) equation 1 in their paper.</p>

<p>The first term in the RHS is always positive, so the second term is the lower bound of the equation, and by that of <script type="math/tex">p(\textbf{x})</script>, the evidence. For this reason <script type="math/tex">L(\phi; \textbf{x})</script> is known as the Evidence Lower BOund or ELBO.</p>

<h1 id="back-on-track">Back on Track</h1>

<p>Recall our original goal, we wanted to maximize <script type="math/tex">p(\textbf{x})</script>, but we couldn’t write it in a tractable form. The above equation means instead of directly maximizing <script type="math/tex">p(\textbf{x})</script>, we can maximize its lower bound.</p>

<script type="math/tex; mode=display">\begin{aligned}
\max_\phi L(\phi; \textbf{x})
\end{aligned}</script>

<p>We can now join both of our goals, learn <script type="math/tex">\theta</script> for the generation process together with learning <script type="math/tex">\phi</script> for <script type="math/tex">p_\theta(\textbf{x} \mid \textbf{z})</script>., the posterior .</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\max_{\theta,\phi} L(\theta,\phi; \textbf{x}) &= \int_{z} q_\phi (\textbf{z} \mid \textbf{x}) \log \frac{p(\textbf{z})p_\theta(\textbf{x} \mid \textbf{z})}{q_\phi(\textbf{z} \mid \textbf{x})}dz\\
&= \mathbb{E}_{q_\phi (\textbf{z} \mid \textbf{x})} [-\log q_\phi(\textbf{z} \mid \textbf{x}) +\log p_\theta(\textbf{x}, \textbf{z})]
\end{aligned} %]]></script>

<p><img src="/assets/understanding-VAE/enc-dec.png" alt="" /></p>

<p><script type="math/tex">L(\theta,\phi; \textbf{x})</script> is now expressed as an estimation. Using Monte Carlo estimate we can estimate it as its average: Generate <script type="math/tex">L</script> different <script type="math/tex">\textbf{z}</script> samples and average the inner function of <script type="math/tex">L(\theta,\phi; \textbf{x})</script></p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\widetilde{L}(\theta,\phi; x) &= \frac{1}{L} \sum_{l=1}^L \log p_\theta(\textbf{x}, \textbf{z}^{(l)}) - \log q_\phi(\textbf{z}^{(l)} \mid \textbf{x})\\
\text{where}~ \textbf{z}^{(l)} &\sim q_\phi (\textbf{z} \mid \textbf{x})
\end{aligned} %]]></script>

<p>Finally we have a function we can optimize, recall gradient decent algorithms optimization step (<script type="math/tex">w_{i+1} = w_i - \lambda\nabla L(w_i)</script> for some rate <script type="math/tex">\lambda</script>), we now need to compute the gradient of <script type="math/tex">L(\theta,\phi; \textbf{x})</script> with respect to both <script type="math/tex">\theta</script> and <script type="math/tex">\phi</script>.</p>

<p>The gradient with respect to <script type="math/tex">\theta</script>, <script type="math/tex">\nabla_\theta</script>,  is fairly straight forward because we can push the gradient inside the sum:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\nabla_\theta\widetilde{L}(\theta,\phi; x) =& \frac{1}{L} \sum_{l=1}^L \nabla_\theta\log q_\phi(\textbf{z}^{(l)} \mid \textbf{x})\\
\text{where}~& \textbf{z}^{(l)} \sim q_\phi (\textbf{z} \mid \textbf{x})
\end{aligned} %]]></script>

<p>Doing the same for <script type="math/tex">\nabla_\phi</script> is not possible as we sample <script type="math/tex">\textbf{z}^{(l)}</script> from <script type="math/tex">q_\phi (\textbf{z} \mid \textbf{x})</script> which depends on <script type="math/tex">\phi</script>. While we can find a gradient estimate using the Log Derivative Trick, empirically, these gradients are too large.</p>

<p>Why does it matter? In optimization, the variance of the gradient is more important than the value of the function itself. The gradient impacts the practicability and the speed of the convergence of the optimization problem, so this is not quite the solution we were looking for. (Interestingly though, the result of the gradient using the Log Derivative Trick is just like REINFORCE, sometimes also called the likelihood ratio)</p>

<p>If we can’t push the gradient inside the sum because <script type="math/tex">\textbf{z}^{(l)}</script> is sampled from <script type="math/tex">q_\phi (\textbf{z} \mid \textbf{x})</script>, can we sample <script type="math/tex">\textbf{z}^{(l)}</script> independently of <script type="math/tex">\phi</script> while still <script type="math/tex">\textbf{z}^{(l)} \sim q_\phi (\textbf{z} \mid \textbf{x})</script>? What Kingma et al. suggested as a solution, known as the reparameterization trick, is to sample <script type="math/tex">\textbf{z} \sim q_\phi (\textbf{z} \mid \textbf{x})</script> using a new auxiliary parameter-free variable <script type="math/tex">\epsilon \sim p(\epsilon)</script> (<script type="math/tex">\epsilon</script> is a vector, having trouble bolding it), and a parametric function <script type="math/tex">g_\phi(\cdot)</script>, such that now <script type="math/tex">\textbf{z}</script> is a deterministic variable <script type="math/tex">\textbf{z} = g_\phi(\epsilon, \textbf{z})</script>. This means, we can write our objective function now as:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\widetilde{L}(\theta,\phi; x) &= \frac{1}{L} \sum_{l=1}^L \log p_\theta(\textbf{x}, \textbf{z}^{(l)}) - \log q_\phi(\textbf{z}^{(l)} \mid \textbf{x})\\
\text{where}~ \textbf{z}^{(l)}&=g_\phi(\epsilon^{(l)}, \textbf{x})~\text{and}~\epsilon^{(l)} \sim p (\epsilon)
\end{aligned} %]]></script>

<p>To put it in a practical example, assume <script type="math/tex">\textbf{z} \sim q_\phi (\textbf{z} \mid \textbf{x}) = \mathcal{N}(\phi_\mu, \phi_\sigma^2)</script>. The reparameterization trick suggests to define <script type="math/tex">\epsilon \sim \mathcal{N}(\textbf{0},\textbf{I})</script> and <script type="math/tex">g_\phi(\epsilon, \textbf{z}) = \mu + \sigma^2\odot\epsilon</script>. Where <script type="math/tex">\odot</script> signify an element-wise product. Now, <script type="math/tex">\textbf{z}^{(l)}</script> can be written as <script type="math/tex">g_\phi(\epsilon^{(l)}, \textbf{x})</script>. Essentially, this reparameterization trick reduces the variance dramatically.</p>

<p>Another novelty the authors suggested in order to further reduce the variance of the gradient is to write <script type="math/tex">L(\theta,\phi; \textbf{x})</script> differently.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
    L(\theta,\phi; \textbf{x}) &= \mathbb{E}_{q_\phi (\textbf{z} \mid \textbf{x})} [\log p_\theta(\textbf{x}, \textbf{z}) - \log q_\phi(\textbf{z} \mid \textbf{x})]\\
&= \mathbb{E}_{q_\phi (\textbf{z} \mid \textbf{x})} [\log p_\theta(\textbf{x} \mid \textbf{z})p_\theta(\textbf{z})- \log q_\phi(\textbf{z} \mid \textbf{x})]\\
&= \mathbb{E}_{q_\phi (\textbf{z} \mid \textbf{x})} [\log p_\theta(\textbf{x} \mid \textbf{z}) + \log p_\theta(\textbf{z})- \log q_\phi(\textbf{z} \mid \textbf{x})]\\
&= \mathbb{E}_{q_\phi (\textbf{z} \mid \textbf{x})} [\log p_\theta(\textbf{x} \mid \textbf{z})] + \mathbb{E}_{q_\phi (\textbf{z} \mid \textbf{x})} [\log p_\theta(\textbf{z}) -\log q_\phi(\textbf{z} \mid \textbf{x})]\\
&= \mathbb{E}_{q_\phi (\textbf{z} \mid \textbf{x})} [\log p_\theta(\textbf{x} \mid \textbf{z})] - \mathbb{E}_{q_\phi (\textbf{z} \mid \textbf{x})} [\log q_\phi(\textbf{z} \mid \textbf{x}) - \log p_\theta(\textbf{z})]\\
&= \mathbb{E}_{q_\phi (\textbf{z} \mid \textbf{x})} [ \log p_{\theta}(\textbf{x} \mid \textbf{z})] - \mathbb{E}_{q_\phi (\textbf{z} \mid \textbf{x})}  [\log \frac{q_\phi(\textbf{z} \mid \textbf{x})}{p_{\theta}(\textbf{z})}]\\
&= \mathbb{E}_{q_\phi (\textbf{z} \mid \textbf{x})} [ \log p_{\theta}(\textbf{x} \mid \textbf{z})] - \int_{z} q_\phi(\textbf{z} \mid \textbf{x}) \log \frac{q_\phi(\textbf{z} \mid \textbf{x})}{p_{\theta}(\textbf{z})}\\
&= \mathbb{E}_{q_\phi (\textbf{z} \mid \textbf{x})} [ \log p_{\theta}(\textbf{x} \mid \textbf{z})] - D_{KL}(q_\phi(\textbf{z} \mid \textbf{x}) \Vert p_{\theta}(\textbf{z}))
\end{aligned} %]]></script>

<p>That is: 
<script type="math/tex">% <![CDATA[
\begin{aligned}
L(\theta,\phi; \textbf{x}) &= \mathbb{E}_{q_\phi (\textbf{z} \mid \textbf{x})} [ \log p_{\theta}(\textbf{x} \mid \textbf{z})] - D_{KL}(q_\phi(\textbf{z} \mid \textbf{x}) \Vert p_{\theta}(\textbf{z}))\\
\text{where}~ \textbf{z}^{(l)}&=g_\phi(\epsilon^{(l)}, \textbf{x})~\text{and}~\epsilon^{(l)} \sim p (\epsilon)
\end{aligned} %]]></script></p>

<p>Again, empirically, this variation shows less variance than the previous way. A motivation for this can be that unlike in the previous definition of <script type="math/tex">L(\theta,\phi; \textbf{x})</script>, here only the first term is an expectation and needs to be assessed using Monte Carlo estimate, the second term can be integrated analytically.</p>

<h1 id="variational-autoencoder">Variational Autoencoder</h1>

<p>Alright, lets tie this all together! From the previous section we finalized our  objective function for both the encoder and the decoder (Previous equation). First lets try to understand it, recall Eq. (1), where we defined a loss function over the reconstruction process. In variational autoencoders we still care about this reconstruction, manifested in <script type="math/tex">\mathbb{E}_{q_\phi (\textbf{z} \mid \textbf{x})} [ \log p_{\theta}(\textbf{x} \mid \textbf{z})]</script>. But we also want to regularize <script type="math/tex">q_\phi (\textbf{z} \mid \textbf{x})</script> by keeping it close to the latent variable distribution, <script type="math/tex">p_{\theta}(\textbf{z})</script>.</p>

<p><img src="/assets/understanding-VAE/var-enc-dec.png" alt="" />
<br /><em>The Variational Autoencoder</em></p>

<p>Now, an example:</p>

<p>Assume for this example, all distributions are Gaussian:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
p_\theta(\textbf{z}) &\sim \mathcal{N}(\textbf{0}, \textbf{I})~\text{Notice}~p_\theta(\textbf{z})~\text{is parameter-free}\\
p_\theta(\textbf{x} \mid \textbf{z}) &\sim \mathcal{N}(\theta_\mu, \theta_{\sigma^2}\textbf{I})\\
q_\phi(\textbf{z} \mid \textbf{x}) &\sim \mathcal{N}(\phi_\mu, \phi_{\sigma^2}\textbf{I})
\end{aligned} %]]></script>

<p>Using this distributions maximize:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
L(\theta,\phi; \textbf{x}) &= \mathbb{E}_{q_\phi (\textbf{z} \mid \textbf{x})} [ \log p_{\theta}(\textbf{x} \mid \textbf{z})] - D_{KL}(q_\phi(\textbf{z} \mid \textbf{x}) \Vert p_{\theta}(\textbf{z}))\\
&= \frac{1}{L} \sum_{l=1}^L \log p_{\theta}(\textbf{x} \mid \textbf{z}^{(l)})) - D_{KL}(q_\phi(\textbf{z} \mid \textbf{x}) \Vert p_{\theta}(\textbf{z}))\\
&= \frac{1}{L} \sum_{l=1}^L \log p_{\theta}(\textbf{x} \mid \textbf{z}^{(l)})) + \frac{1}{2}\sum_{j=1}^J(1+\log((\sigma^{(i)})^2)-(\mu^{(i)})^2-(\sigma^{(i)})^2) \\
\text{where}~ \textbf{z}^{(l)}&=g_\phi(\epsilon^{(l)}, \textbf{x})~\text{and}~\epsilon \sim p (\epsilon)
\end{aligned} %]]></script>

<p>I did promise some practical point of view to this: The corresponding objective function is: (You can find the full Pytorch code  \href{https://github.com/pytorch/examples/blob/master/vae/main.py}{here})</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">loss_function</span><span class="p">(</span><span class="n">recon_x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">):</span>
    <span class="n">BCE</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy</span><span class="p">(</span><span class="n">recon_x</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'sum'</span><span class="p">)</span>
    <span class="n">KLD</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">logvar</span> <span class="o">-</span> <span class="n">mu</span><span class="o">.</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">logvar</span><span class="o">.</span><span class="n">exp</span><span class="p">())</span>

    <span class="k">return</span> <span class="n">BCE</span> <span class="o">+</span> <span class="n">KLD</span>
</code></pre></div></div>

<p>As mentioned before, we can use MLP as the approximator to all our functions:</p>

<p>Encoder:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\log q_\phi(\textbf{z} \mid \textbf{x}) &= \log \mathcal{N}(\phi_\mu, \phi_{\sigma^2}\textbf{I)}\\
\text{where}\\
\phi_\mu &= \phi_{W_1}h + \phi_{b_1}\\
\log \phi_{\sigma^2} &= \phi_{W_2}h + \phi_{b_2}\\
\phi_h &= \tanh{\phi_{W_3}x+\phi_{b_3}}
\end{aligned} %]]></script>

<p>decoder:
<script type="math/tex">% <![CDATA[
\begin{aligned}
\log p_\theta(\textbf{x} \mid \textbf{z}) &= \log \mathcal{N}(\theta_\mu, \theta_{\sigma^2}\textbf{I)}\\
\text{where}\\
\theta_\mu &= \theta_{W_1}h + \theta_{b_1}\\
\log \theta_{\sigma^2} &= \theta_{W_2}h + \theta_{b_2}\\
\theta_h &= \tanh{\theta_{W_3}z+\theta_{b_3}}
\end{aligned} %]]></script></p>

<p>So the corresponding Encoder and Decoder are:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">VAE</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VAE</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc21</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc22</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">h1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc21</span><span class="p">(</span><span class="n">h1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc22</span><span class="p">(</span><span class="n">h1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reparameterize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">std</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.5</span><span class="o">*</span><span class="n">logvar</span><span class="p">)</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">std</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">eps</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">std</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">mu</span>

    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="n">h3</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc4</span><span class="p">(</span><span class="n">h3</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">))</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reparameterize</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span>
</code></pre></div></div>

<h1 id="references">References</h1>

<p>[1] <a href="https://arxiv.org/pdf/1312.6114.pdf">https://arxiv.org/pdf/1312.6114.pdf</a></p>

<p>[2] <a href="http://gokererdogan.github.io/2017/08/15/variational-autoencoder-explained/">http://gokererdogan.github.io/2017/08/15/variational-autoencoder-explained/</a></p>

<p>[3] <a href="https://icml.cc/2012/papers/687.pdf">https://icml.cc/2012/papers/687.pdf</a></p>

<p>[4] <a href="https://www.youtube.com/watch?v=ogdv\_6dbvVQ">https://www.youtube.com/watch?v=ogdv_6dbvVQ</a></p>

<p>[5] <a href="https://blog.evjang.com/2016/08/variational-bayes.html">https://blog.evjang.com/2016/08/variational-bayes.html</a></p>

<p>[6] <a href="https://xyang35.github.io/2017/04/14/variational-lower-bound/">https://xyang35.github.io/2017/04/14/variational-lower-bound/</a></p>

<p>[7] <a href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/">https://jaan.io/what-is-variational-autoencoder-vae-tutorial/</a></p>

<p>[8] <a href="http://andymiller.github.io/2016/12/19/elbo-gradient-estimators.html">http://andymiller.github.io/2016/12/19/elbo-gradient-estimators.html</a></p>

<p>[9] <a href="https://arxiv.org/pdf/1601.00670.pdf">https://arxiv.org/pdf/1601.00670.pdf</a></p>

<p>[10] <a href="https://github.com/pytorch/examples/blob/master/vae/main.py">https://github.com/pytorch/examples/blob/master/vae/main.py</a></p>

<p>[11] <a href="Pattern Recognition and Machine Learning, Bishop (2006)">http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf</a></p>

  
  </section>
  
  <!-- Social media shares -->
  <div class="share-buttons">
    <ul class="share-buttons">
        <div class="meta">Share</div>
        
        <li>
            <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/2018/10/26/understanding-variational-autoencoders.html" target="_blank" title="Share on Facebook">
			<i class="fa fa-facebook-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Share on Facebook</span>
		</a>
        </li>
         
        <li>
            <a href="https://twitter.com/intent/tweet?source=http://localhost:4000/2018/10/26/understanding-variational-autoencoders.html&text=Understanding+Variational+Autoencoders%20%7C%20Matan+Eyal:%20http://localhost:4000/2018/10/26/understanding-variational-autoencoders.html" target="_blank" title="Tweet">
			<i class="fa fa-twitter-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Tweet</span>
		</a>
        </li>
                
        <li>
            <a href="mailto:?subject=Understanding+Variational+Autoencoders%20%7C%20Matan+Eyal&body=:%20http://localhost:4000/2018/10/26/understanding-variational-autoencoders.html" target="_blank" title="Email">
			<i class="fa fa-envelope-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Email</span>
		</a>
        </li>
        
    </ul>
</div>
   
   <!-- Tag list -->
  
  


<footer>
  <div class="tag-list"></div>
</footer>

    
</article>

<!-- Disqus -->


<!-- Post navigation -->

  <div id="post-nav">
  
  <div id="previous-post" class="post-nav-post">
      <p>Previous post</p>
      <a href="/2018/06/24/implementation-flow-of-neural-networks.html">
        Implementation Flow Of Neural Networks
      </a>
  </div>
  
  
</div>

    </div>
    
<footer class="site-footer">
    <p class="text">Powered by <a href="https://jekyllrb.com/">Jekyll</a> with <a href="https://github.com/sylhare/Type-on-Strap">Type on Strap</a>
</p>
            <div class="footer-icons">
                <ul>
                <!-- Social icons from Font Awesome, if enabled -->
                
<li>
	<a href="http://localhost:4000/feed.xml" title="Follow RSS feed">
		<span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>



<li>
	<a href="mailto:mataneyal1@gmail.com" title="Email">
		<span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-envelope fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>













<li>
	<a href="https://github.com/mataney" title="Follow on GitHub">
		<span class="fa-stack fa-lg">
            <i class="fa fa-circle fa-stack-2x"></i>
            <i class="fa fa-github fa-stack-1x fa-inverse"></i>
        </span>
	</a>
</li>
































                </ul>
            </div>
</footer>




  </body>
</html>
